# -*- coding: utf-8 -*-
"""Amazon Fashion Products.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z3mO5sMzi5ohkaQrXW1gOr_iSzQnanAJ
"""

import pandas as pd
from google.colab import files

# Upload file
uploaded = files.upload()

# Get file name from uploaded dictionary
file_name = list(uploaded.keys())[0]

# Read CSV (or Excel)
if file_name.endswith(".csv"):
    df = pd.read_csv(file_name)
else:
    df = pd.read_excel(file_name)

# Show info
print("Dataset shape:", df.shape)
print("\nSample rows:")
print(df.head())
print("\nDataFrame Info:")
print(df.info())

# Number of rows and columns
rows, cols = df.shape
print(f"Number of rows: {rows}")
print(f"Number of columns: {cols}")

# Print all column names
print("Column names:")
print(df.columns.tolist())

# Number of unique brands
print("Number of unique brands:", df['brand'].nunique())

# List of unique brands
print("\nUnique brand names:")
print(df['brand'].unique())

# Brand counts (how many products per brand)
print("\nBrand value counts:")
print(df['brand'].value_counts())

import matplotlib.pyplot as plt
import seaborn as sns

# Count products per brand
brand_counts = df['brand'].value_counts().head(10)   # top 10 brands

# Plot
plt.figure(figsize=(10,6))
sns.barplot(x=brand_counts.values, y=brand_counts.index, palette="viridis")
plt.title("Top 10 Brands by Number of Products")
plt.xlabel("Number of Products")
plt.ylabel("Brand")
plt.show()

# Count of null values in brand column
print("Number of null values in 'brand':", df['brand'].isnull().sum())

# Show rows where brand is missing (if any)
print("\nRows with missing brand values:")
print(df[df['brand'].isnull()])

# Count of null values in price column
print("Number of null values in 'price':", df['price'].isnull().sum())

# Show rows where price is missing (if any)
print("\nRows with missing price values:")
print(df[df['price'].isnull()])

import matplotlib.pyplot as plt

plt.figure(figsize=(8,5))
# Drop null values before plotting
plt.hist(df['price'].dropna(), bins=100, color='skyblue', edgecolor='black')
plt.xlabel('Price')
plt.ylabel('Number of Rows')
plt.title('Distribution of Price Values (excluding Nulls)')
plt.show()

# Count of null values in Category column
print("Number of null values in 'Category':", df['category'].isnull().sum())

# Show rows where Category is missing (if any)
print("\nRows with missing Category values:")
print(df[df['category'].isnull()])

import matplotlib.pyplot as plt

# Count of each category
category_counts = df['category'].value_counts()

plt.figure(figsize=(12,6))
category_counts.plot(kind='bar', color='skyblue', edgecolor='black')
plt.xlabel('category')
plt.ylabel('Number of Rows')
plt.title('Count of Each Category')
plt.xticks(rotation=45)
plt.show()

# Number of unique categories
num_categories = df['category'].nunique()
print("Number of different categories:", num_categories)

# List of unique categories
print("Categories:", df['category'].unique())

# Count of null values in rating column
print("Number of null values in 'rating':", df['rating'].isnull().sum())

# Show rows where rating is missing (if any)
print("\nRows with missing rating values:")
print(df[df['rating'].isnull()])

import matplotlib.pyplot as plt

# Count of each rating, including nulls
rating_counts = df['rating'].value_counts(dropna=False)

# Replace NaN index with string 'Null' for plotting
rating_counts.index = rating_counts.index.fillna('Null')

plt.figure(figsize=(10,5))
rating_counts.plot(kind='bar', color='skyblue', edgecolor='black')
plt.xlabel('Rating')
plt.ylabel('Number of Rows')
plt.title('Distribution of Rating Values (including Nulls)')
plt.show()

# Count of null values in product_url column
print("Number of null values in 'product_url':", df['product_url'].isnull().sum())

# Show rows where product_url is missing (if any)
print("\nRows with missing product_url values:")
print(df[df['product_url'].isnull()])

# Count of null values in image_url column
print("Number of null values in 'image_url':", df['image_url'].isnull().sum())

# Show rows where image_url is missing (if any)
print("\nRows with missing image_url values:")
print(df[df['image_url'].isnull()])

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12,6))

# Boxplot: Rating vs Category
sns.boxplot(x='category', y='rating', data=df)
plt.xticks(rotation=45)
plt.xlabel('Category')
plt.ylabel('Rating')
plt.title('Distribution of Ratings Across Categories')
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Cross-tabulation of brand vs category
brand_category = pd.crosstab(df['brand'], df['category'])

# Heatmap
plt.figure(figsize=(12,8))
sns.heatmap(brand_category, annot=True, fmt='d', cmap='YlGnBu')
plt.title('Number of Products per Brand and Category')
plt.xlabel('Category')
plt.ylabel('Brand')
plt.show()

# Pivot table: brand vs category, with average rating
brand_category_rating = df.pivot_table(
    index='brand',
    columns='category',
    values='rating',
    aggfunc='mean'
)

plt.figure(figsize=(12,8))
sns.heatmap(brand_category_rating, annot=True, fmt='.2f', cmap='coolwarm')
plt.title('Average Rating per Brand and Category')
plt.xlabel('Category')
plt.ylabel('Brand')
plt.show()

plt.figure(figsize=(12,6))
sns.boxplot(x='brand', y='rating', data=df)
plt.xticks(rotation=45)
plt.title('Rating Distribution per Brand')
plt.show()

# Select rows where brand is NaN
nan_brand_rows = df[df['brand'].isna()]

# Display the result
print(nan_brand_rows)

import pandas as pd

# Function to fill NaN brand with the most frequent brand in the same category
def fill_brand(row):
    if pd.isna(row['brand']):
        # Get the most common brand in this category
        most_common_brand = df[df['category'] == row['category']]['brand'].mode()
        if not most_common_brand.empty:
            return most_common_brand[0]  # return the top brand
        else:
            return row['brand']  # if no brand exists, keep NaN
    else:
        return row['brand']

# Apply the function
df['brand'] = df.apply(fill_brand, axis=1)

# Check the rows that were filled
nan_filled_rows = df[df['brand'].notna()]
print(nan_filled_rows)

# Select rows where brand is NaN
nan_brand_rows = df[df['brand'].isna()]

# Display the result
print(nan_brand_rows)

# Cross-tabulation of brand vs category
brand_category_count = pd.crosstab(df['brand'], df['category'])

plt.figure(figsize=(12,8))
sns.heatmap(brand_category_count, annot=True, fmt='d', cmap='YlGnBu')
plt.title('Number of Products per Brand and Category')
plt.xlabel('Category')
plt.ylabel('Brand')
plt.show()

plt.figure(figsize=(12,6))
sns.boxplot(x='brand', y='rating', data=df)
plt.xticks(rotation=45)
plt.title('Rating Distribution per Brand')
plt.show()

import pandas as pd

# Compute mean rating per brand and category
mean_rating = df.groupby(['brand', 'category'])['rating'].mean().reset_index()
mean_rating = mean_rating.rename(columns={'rating': 'mean_rating'})

# Merge mean_rating with original df
df_filled = df.merge(mean_rating, on=['brand', 'category'], how='left')

# Fill NaN rating with the mean_rating
df_filled['rating'] = df_filled['rating'].fillna(df_filled['mean_rating'])

# Drop the helper column
df_filled = df_filled.drop(columns=['mean_rating'])

# Check rows that had null ratings (before filling)
null_ratings_filled = df_filled[df_filled['rating'].notna()]
print(null_ratings_filled)

# Count of null values in the 'rating' column
null_ratings_count = df_filled['rating'].isnull().sum()
print("Number of null values in 'rating' column:", null_ratings_count)

# List of all column names
column_names = df.columns.tolist()
print("Columns in the dataset:", column_names)

# Select rows where rating is null
null_rating_rows = df[df['rating'].isna()]

# Display the result
print(null_rating_rows)

import pandas as pd

# Compute mean rating per category as a dictionary
category_mean_rating = df.groupby('category')['rating'].mean().to_dict()

# Fill null ratings with category mean
df['rating'] = df.apply(
    lambda row: category_mean_rating[row['category']] if pd.isna(row['rating']) else row['rating'],
    axis=1
)

# Verify that there are no more null ratings
print(df[df['rating'].isna()])

# Count of null values in 'rating' column
null_ratings_count = df['rating'].isna().sum()
print("Number of NaN values in 'rating' column:", null_ratings_count)

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10,6))
sns.histplot(df['rating'], bins=20, kde=True, color='skyblue')
plt.title('Distribution of Ratings')
plt.xlabel('Rating')
plt.ylabel('Count')
plt.show()

# Check if there are any null values
has_null = df['rating'].isnull().any()
print("Are there any null values in 'rating' column?", has_null)

# Optional: count them
null_count = df['rating'].isnull().sum()
print("Number of null values in 'rating' column:", null_count)

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12,6))
sns.boxplot(x='category', y='rating', data=df, palette='pastel')
plt.xticks(rotation=45)
plt.title('Rating Distribution per Category')
plt.xlabel('Category')
plt.ylabel('Rating')
plt.show()

plt.figure(figsize=(12,6))
sns.violinplot(x='category', y='rating', data=df, palette='muted')
plt.xticks(rotation=45)
plt.title('Rating Density per Category')
plt.xlabel('Category')
plt.ylabel('Rating')
plt.show()

plt.figure(figsize=(12,6))
sns.barplot(x='category', y='rating', data=df, estimator=lambda x: x.mean(), ci=None, palette='coolwarm')
plt.xticks(rotation=45)
plt.title('Average Rating per Category')
plt.xlabel('Category')
plt.ylabel('Average Rating')
plt.show()

# Count null values per column
null_counts = df.isnull().sum()

# Filter columns that have at least one null value
columns_with_nulls = null_counts[null_counts > 0].index.tolist()

print("Columns with null values:", columns_with_nulls)

# Count of null values in 'title' column
null_title_count = df['title'].isnull().sum()
print("Number of null values in 'title' column:", null_title_count)

import pandas as pd

# Function to fill null title with mode of the same category and brand
def fill_title(row):
    if pd.isna(row['title']):
        # Get mode title for the same brand and category
        mode_title = df[(df['brand'] == row['brand']) & (df['category'] == row['category'])]['title'].mode()
        if not mode_title.empty:
            return mode_title[0]
        else:
            return row['title']  # If no mode exists, keep NaN
    else:
        return row['title']

# Apply the function
df['title'] = df.apply(fill_title, axis=1)

# Verify if any nulls remain in title
print(df['title'].isnull().sum())

# Count of null values in 'price' column
null_price_count = df['price'].isnull().sum()
print("Number of null values in 'price' column:", null_price_count)

import pandas as pd

# Compute mean price per brand and category
mean_price = df.groupby(['brand', 'category'])['price'].mean().reset_index()
mean_price = mean_price.rename(columns={'price': 'mean_price'})

# Merge mean_price with original df
df = df.merge(mean_price, on=['brand', 'category'], how='left')

# Fill null price values with the mean price
df['price'] = df['price'].fillna(df['mean_price'])

# Drop the helper column
df = df.drop(columns=['mean_price'])

# Verify no more nulls in price
print("Number of null values in 'price' column:", df['price'].isnull().sum())

import pandas as pd

# Compute mean price per category as a dictionary
category_mean_price = df.groupby('category')['price'].mean().to_dict()

# Fill null price values with category mean
df['price'] = df.apply(
    lambda row: category_mean_price[row['category']] if pd.isna(row['price']) else row['price'],
    axis=1
)

# Verify no more nulls in price
print("Number of null values in 'price' column:", df['price'].isnull().sum())

# Count null values per column
null_counts = df.isnull().sum()

# Filter columns that have at least one null value
columns_with_nulls = null_counts[null_counts > 0].index.tolist()

print("Columns with null values:", columns_with_nulls)

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12,6))
sns.boxplot(x='category', y='price', data=df, palette='pastel')
plt.xticks(rotation=45)
plt.title('Price Distribution per Category')
plt.xlabel('Category')
plt.ylabel('Price')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(14,7))
sns.boxplot(x='brand', y='price', hue='category', data=df)
plt.xticks(rotation=45)
plt.title('Price Distribution per Brand grouped by Category')
plt.xlabel('Brand')
plt.ylabel('Price')
plt.legend(bbox_to_anchor=(1.05, 1), loc=2)
plt.show()

import pandas as pd
import requests
from PIL import Image
from io import BytesIO
import matplotlib.pyplot as plt


# Clean the 'image_url' column (remove malformed part)
df['image_url'] = df['image_url'].str.replace("https://m. ", "", regex=False)

# Function to show images
def show_images_from_urls(urls, titles=None):
    num_images = len(urls)
    plt.figure(figsize=(15, 5))

    for i, url in enumerate(urls):
        try:
            response = requests.get(url, timeout=5)
            img = Image.open(BytesIO(response.content))
            plt.subplot(1, num_images, i + 1)
            plt.imshow(img)
            plt.axis('off')
            if titles:
                plt.title(titles[i][:30], fontsize=8)
        except Exception as e:
            print(f"Error loading image {i+1}: {e}")

    plt.tight_layout()
    plt.show()

# Select a few rows to show
sample = df.head(10)  # Show first 5 images
show_images_from_urls(sample['image_url'].tolist(), sample['title'].tolist())

import pandas as pd
import requests
from PIL import Image
from io import BytesIO
import matplotlib.pyplot as plt


# Clean the 'image_url' column (remove malformed part)
df['image_url'] = df['image_url'].str.replace("https://m. ", "", regex=False)

# Function to show images
def show_images_from_urls(urls, titles=None):
    num_images = len(urls)
    plt.figure(figsize=(15, 5))

    for i, url in enumerate(urls):
        try:
            response = requests.get(url, timeout=5)
            img = Image.open(BytesIO(response.content))
            plt.subplot(1, num_images, i + 1)
            plt.imshow(img)
            plt.axis('off')
            if titles:
                plt.title(titles[i][:30], fontsize=8)
        except Exception as e:
            print(f"Error loading image {i+1}: {e}")

    plt.tight_layout()
    plt.show()

# Select a few rows to show
sample = df.tail(10)  # Show first 5 images
show_images_from_urls(sample['image_url'].tolist(), sample['title'].tolist())

import pandas as pd
from IPython.display import display, HTML



# Clean the URL columns (remove malformed text if needed)
df['product_url'] = df['product_url'].str.replace("https://m. ", "", regex=False)
df['image_url'] = df['image_url'].str.replace("https://m. ", "", regex=False)

# Show first N products
n = 5
sample = df.head(n)

# Generate HTML to display product info with image and clickable link
html_rows = []
for _, row in sample.iterrows():
    product_html = f"""
    <div style='margin-bottom: 30px; border: 1px solid #ccc; padding: 10px; width: 100%; display: flex; gap: 20px;'>
        <div>
            <img src="{row['image_url']}" alt="Product Image" style="max-width:150px; max-height:150px;">
        </div>

    </div>
    """
    html_rows.append(product_html)

# Display in Jupyter Notebook or Google Colab
display(HTML(''.join(html_rows)))

import pandas as pd
from IPython.display import display, HTML


# Clean the URL columns (remove malformed text if needed)
df['product_url'] = df['product_url'].str.replace("https://m. ", "", regex=False)
df['image_url'] = df['image_url'].str.replace("https://m. ", "", regex=False)

# Show first N products
n = 5
sample = df.head(n)

# Generate HTML to display product info with image and clickable link
html_rows = []
for _, row in sample.iterrows():
    product_html = f"""
    <div style='margin-bottom: 30px; border: 1px solid #ccc; padding: 10px; width: 100%; display: flex; gap: 20px;'>
        <div>
            <img src="{row['image_url']}" alt="Product Image" style="max-width:150px; max-height:150px;">
        </div>
        <div>
            <b>Category:</b> {row['category']}<br>


        </div>
    </div>
    """
    html_rows.append(product_html)

# Display in Jupyter Notebook or Google Colab
display(HTML(''.join(html_rows)))

import pandas as pd
from IPython.display import display, HTML


# Clean the URL columns (remove malformed text if needed)
df['product_url'] = df['product_url'].str.replace("https://m. ", "", regex=False)
df['image_url'] = df['image_url'].str.replace("https://m. ", "", regex=False)

# Show first N products
n = 5
sample = df.tail(n)

# Generate HTML to display product info with image and clickable link
html_rows = []
for _, row in sample.iterrows():
    product_html = f"""
    <div style='margin-bottom: 30px; border: 1px solid #ccc; padding: 10px; width: 100%; display: flex; gap: 20px;'>
        <div>
            <img src="{row['image_url']}" alt="Product Image" style="max-width:150px; max-height:150px;">
        </div>
        <div>
            <b>Category:</b> {row['category']}<br>


        </div>
    </div>
    """
    html_rows.append(product_html)

# Display in Jupyter Notebook or Google Colab
display(HTML(''.join(html_rows)))

# Save DataFrame to CSV (overwrite if file exists)
df.to_csv("products.csv", index=False)

print("✅ Data saved as products.csv")

import pandas as pd
from google.colab import files

# Upload file
uploaded = files.upload()

# Get file name from uploaded dictionary
file_name = list(uploaded.keys())[0]

# Read CSV (or Excel)
if file_name.endswith(".csv"):
    df = pd.read_csv(file_name)
else:
    df = pd.read_excel(file_name)

# Show info
print("Dataset shape:", df.shape)
print("\nSample rows:")
print(df.head())
print("\nDataFrame Info:")
print(df.info())

# Count null values per column
null_counts = df.isnull().sum()

# Filter columns that have at least one null value
columns_with_nulls = null_counts[null_counts > 0].index.tolist()

print("Columns with null values:", columns_with_nulls)

import pandas as pd
import os
import requests
from PIL import Image
from io import BytesIO
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.utils import to_categorical

import os
import pandas as pd
import numpy as np
import requests
from PIL import Image
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed
from sklearn.preprocessing import LabelEncoder

# Parameters
IMG_SIZE = 128
OUTPUT_DIR = "downloaded_images"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# --- Load your dataframe here ---
# Example: df = pd.read_csv("data.csv")

# Clean URLs and encode labels
df['image_url'] = df['image_url'].str.strip()
df['product_url'] = df['product_url'].str.strip()

label_encoder = LabelEncoder()
df['label'] = label_encoder.fit_transform(df['category'])

def download_image_safe(idx, row):
    """Download and resize image safely."""
    url = row['image_url']
    try:
        response = requests.get(url, timeout=5)
        response.raise_for_status()  # ensure valid response
        img = Image.open(BytesIO(response.content)).convert('RGB')
        img = img.resize((IMG_SIZE, IMG_SIZE))

        # Save to disk (avoid memory explosion)
        save_path = os.path.join(OUTPUT_DIR, f"{idx}_{row['label']}.jpg")
        img.save(save_path, format="JPEG", quality=90)

        return np.array(img, dtype=np.uint8), row['label'], row['product_url']

    except Exception as e:
        print(f"❌ Failed to process {url}: {e}")
        return None

images, labels, product_urls = [], [], []

with ThreadPoolExecutor(max_workers=5) as executor:  # fewer workers = safer
    futures = [executor.submit(download_image_safe, i, row) for i, row in df.iterrows()]
    for future in as_completed(futures):
        result = future.result()
        if result is not None:
            img, label, product_url = result
            images.append(img)
            labels.append(label)
            product_urls.append(product_url)

images = np.array(images, dtype=np.uint8)
labels = np.array(labels, dtype=np.int32)
product_urls = np.array(product_urls)

print(f"✅ Downloaded {len(images)} images, resized to {IMG_SIZE}x{IMG_SIZE}")
print(f"Labels shape: {labels.shape}")
print(f"Images array size: {images.nbytes / 1024 / 1024:.2f} MB")

# Optional: save as .npy for later use
np.save("images.npy", images)
np.save("labels.npy", labels)
np.save("product_urls.npy", product_urls)

from tensorflow.keras.utils import to_categorical

from sklearn.model_selection import train_test_split

import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

# Normalize images (convert to float32 for efficiency)
X = np.array(images, dtype=np.float32) / 255.0

# One-hot encode labels
y = to_categorical(np.array(labels, dtype=np.int32))

# Train-test split (keep product_urls aligned with data)
X_train, X_test, y_train, y_test, urls_train, urls_test = train_test_split(
    X, y, product_urls, test_size=0.2, random_state=42, stratify=labels
)

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)
print("Example product URL from test set:", urls_test[0])

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

# Image size (must match preprocessing step)
IMG_SIZE = 128

# Define the CNN model
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(y.shape[1], activation='softmax')  # number of categories = y.shape[1]
])

# Compile the model
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Print summary
model.summary()

# Save the complete model in HDF5 format
model.save("cnn_model.h5")
print("✅ Model saved as cnn_model.h5")

import gc
import tensorflow as tf

# Free up memory / clear previous models
gc.collect()
tf.keras.backend.clear_session()

# Create an ImageDataGenerator (no rescale since X_train is already normalized)
datagen = tf.keras.preprocessing.image.ImageDataGenerator()

# Build generator
train_gen = datagen.flow(X_train, y_train, batch_size=32, shuffle=True)

# Fit the model
history = model.fit(
    train_gen,
    epochs=10,
    validation_data=(X_test, y_test),  # track validation accuracy
    verbose=1
)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.regularizers import l2

IMG_SIZE = 128
num_classes = y.shape[1]   # one-hot encoded labels

model = Sequential([
    # Conv Block 1
    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3), kernel_regularizer=l2(1e-4)),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),

    # Conv Block 2
    Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(1e-4)),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),

    # Conv Block 3
    Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(1e-4)),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),

    Flatten(),

    # Dense layers
    Dense(256, activation='relu', kernel_regularizer=l2(1e-4)),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

# Compile model
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

# Save the complete model in HDF5 format
model.save("cnn_model.h5")
print("✅ Model saved as cnn_model.h5")

import gc
import tensorflow as tf

# Free up memory / clear previous models
gc.collect()
tf.keras.backend.clear_session()

# Create an ImageDataGenerator (no rescale since X_train is already normalized)
datagen = tf.keras.preprocessing.image.ImageDataGenerator()

# Build generator
train_gen = datagen.flow(X_train, y_train, batch_size=32, shuffle=True)

# Fit the model
history = model.fit(
    train_gen,
    epochs=10,
    validation_data=(X_test, y_test),  # track validation accuracy
    verbose=1
)

datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

train_gen = datagen.flow(X_train, y_train, batch_size=32, shuffle=True)

callbacks = [
    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)
]

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense, Dropout

# --- Build Model ---
base_model = tf.keras.applications.MobileNetV2(
    input_shape=(IMG_SIZE, IMG_SIZE, 3),
    include_top=False,
    weights='imagenet'
)
base_model.trainable = False  # freeze pretrained layers

model = Sequential([
    base_model,
    Flatten(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

# --- Compile ---
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

# --- Data Augmentation ---
datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

train_gen = datagen.flow(X_train, y_train, batch_size=32, shuffle=True)

# --- Callbacks ---
callbacks = [
    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)
]

# --- Train ---
history = model.fit(
    train_gen,
    epochs=20,
    validation_data=(X_test, y_test),
    callbacks=callbacks,
    verbose=1
)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense, Dropout

# --- Build Model ---
base_model = tf.keras.applications.MobileNetV2(
    input_shape=(IMG_SIZE, IMG_SIZE, 3),
    include_top=False,
    weights='imagenet'
)
base_model.trainable = False  # freeze pretrained layers

model = Sequential([
    base_model,
    Flatten(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

# --- Compile ---
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

# --- Data Augmentation ---
datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

train_gen = datagen.flow(X_train, y_train, batch_size=32, shuffle=True)

# --- Callbacks ---
callbacks = [
    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)
]

# --- Train ---
history = model.fit(
    train_gen,
    epochs=25,
    validation_data=(X_test, y_test),
    callbacks=callbacks,
    verbose=1
)

from tensorflow.keras.preprocessing import image
import numpy as np
from PIL import Image
IMG_SIZE = 128

def preprocess_image(img_path):
    """
    Loads an image, resizes it to IMG_SIZE, converts to array, and normalizes.
    """
    img = Image.open(img_path).convert('RGB')
    img = img.resize((IMG_SIZE, IMG_SIZE))
    img_array = np.array(img, dtype=np.float32) / 255.0  # normalize
    img_array = np.expand_dims(img_array, axis=0)        # add batch dimension
    return img_array

import base64
import numpy as np
from PIL import Image
from io import BytesIO
from tensorflow.keras.models import load_model

# Parameters
IMG_SIZE = 128

# Load your trained model (replace with your model path if needed)
# model = load_model("your_trained_model.h5")

# Example: label_encoder must be the same used during training
# from sklearn.preprocessing import LabelEncoder
# label_encoder = LabelEncoder()
# label_encoder.classes_ = np.load("classes.npy", allow_pickle=True)

# Base64 image string (replace with your actual string)
img_base64 = "data:image/jpeg;base64,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"  # truncated

# --- Decode Base64 and preprocess ---
img_data = base64.b64decode(img_base64.split(',')[1])  # extract and decode
img = Image.open(BytesIO(img_data)).convert('RGB')
img = img.resize((IMG_SIZE, IMG_SIZE))

img_array = np.array(img, dtype=np.float32) / 255.0  # normalize
img_array = np.expand_dims(img_array, axis=0)        # add batch dimension

# --- Predict ---
pred_probs = model.predict(img_array)
pred_class_index = np.argmax(pred_probs)  # most likely class
pred_confidence = pred_probs[0][pred_class_index]

# Map back to original category name
pred_category = label_encoder.inverse_transform([pred_class_index])[0]

print(f"Predicted Category: {pred_category}")
print(f"Confidence: {pred_confidence*100:.2f}%")

import base64
import numpy as np
from PIL import Image
from io import BytesIO
from tensorflow.keras.models import load_model

# Parameters
IMG_SIZE = 128

# Load your trained model (replace with your model path if needed)
# model = load_model("your_trained_model.h5")

# Example: label_encoder must be the same used during training
# from sklearn.preprocessing import LabelEncoder
# label_encoder = LabelEncoder()
# label_encoder.classes_ = np.load("classes.npy", allow_pickle=True)

# Base64 image string (replace with your actual string)
img_base64 = "data:image/jpeg;base64,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"

# --- Decode Base64 and preprocess ---
img_data = base64.b64decode(img_base64.split(',')[1])  # extract and decode
img = Image.open(BytesIO(img_data)).convert('RGB')
img = img.resize((IMG_SIZE, IMG_SIZE))

img_array = np.array(img, dtype=np.float32) / 255.0  # normalize
img_array = np.expand_dims(img_array, axis=0)        # add batch dimension

# --- Predict ---
pred_probs = model.predict(img_array)
pred_class_index = np.argmax(pred_probs)  # most likely class
pred_confidence = pred_probs[0][pred_class_index]

# Map back to original category name
pred_category = label_encoder.inverse_transform([pred_class_index])[0]

print(f"Predicted Category: {pred_category}")
print(f"Confidence: {pred_confidence*100:.2f}%")